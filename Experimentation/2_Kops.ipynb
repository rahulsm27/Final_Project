{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "1 Install Kubectl\n",
    "\n",
    "#https://kubernetes.io/docs/tasks/tools/install-kubectl-macos/#install-kubectl-on-macos\n",
    "\n",
    "curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/darwin/amd64/kubectl\"\n",
    "\n",
    "chmod +x ./kubectl\n",
    "\n",
    "sudo mv ./kubectl /usr/local/bin/kubectl\n",
    "sudo chown root: /usr/local/bin/kubectl\n",
    "\n",
    "kubectl version --client\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2 Install Kops\n",
    "\n",
    "https://kops.sigs.k8s.io/getting_started/install/\n",
    "\n",
    "# #brew install kops\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# 3 Create bucket \n",
    "\n",
    "gsutil mb gs://kubernetes-clusters-mlendtoend\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "kops create cluster creates the Cluster object and InstanceGroup object you'll be working with in kOps:\n",
    "\n",
    "export KOPS_STATE_STORE=gs://kubernetes-clusters-mlendtoend/\n",
    "\n",
    "kops create cluster simple.k8s.local --zones asia-south1-a --state ${KOPS_STATE_STORE}/ --project=${PROJECT}\n",
    "\n",
    "# Change kubernetes version to 1.26.12\n",
    "#Change machine type\n",
    "\n",
    "Suggestions:\n",
    " * list clusters with: kops get cluster\n",
    " * edit this cluster with: kops edit cluster simple.k8s.local\n",
    " * edit your node instance group: kops edit ig --name=simple.k8s.local nodes-asia-south1-a\n",
    " * edit your control-plane instance group: kops edit ig --name=simple.k8s.local control-plane-asia-south1-a\n",
    "\n",
    "Finally configure your cluster with: kops update cluster --name simple.k8s.local --yes --admin\n",
    "\n",
    "kops update cluster simple.k8s.local\n",
    "kops update cluster simple.k8s.local --yes\n",
    "\n",
    "\n",
    "\n",
    "Suggestions:\n",
    " * validate cluster: kops validate cluster --wait 10m\n",
    " * list nodes: kubectl get nodes --show-labels\n",
    " * ssh to a control-plane node: ssh -i ~/.ssh/id_rsa ubuntu@\n",
    "\n",
    "\n",
    "kops delete cluster simple.k8s.local --yes\n",
    "\n",
    "kops export kubeconfig --admin\n",
    "\n",
    "kubectl get nodes\n",
    "kubectl version --short\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "export PIPELINE_VERSION=2.0.5\n",
    "kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION\"\n",
    "kubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io\n",
    "kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref=$PIPELINE_VERSION\"\n",
    "\n",
    "kubectl describe configmap inverse-proxy-config -n kubeflow | grep googleusercontent.com\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "kubectl taint nodes --all node-role.kubernetes.io/control-plane-\n",
    "\n",
    "while ! kustomize build example | kubectl apply -f -; do echo \"Retrying to apply resources\"; sleep 10; done\n",
    "\n",
    "\n",
    "kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80\n",
    "\n",
    "#to kill portoforward\n",
    "lsof -i :8080  \n",
    "kill -9 <pid>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "kubectl taint nodes --all node-role.kubernetes.io/control-plane-\n",
    "\n",
    "# kubectl get nodes\n",
    "# kubectl get all -A\n",
    "# kubectl describe nodes\n",
    "\n",
    "#kubectl get pods -n kubeflow\n",
    "#kubectl describe pods -n kubeflow-dashboard-0 \n",
    "\n",
    "#kubectl logs -n kubeflow kubeflow-dashboard-0 \n",
    "\n",
    "# sudo systemctl restart kubelet\n",
    "# kubectl get services -n kubeflow\n",
    "\n",
    "kubectl label ns admin admin.kubeflow.org/enabled=\"true\"\n",
    "\n",
    "kfp pipeline list\n",
    "\n",
    "kubectl edit configmap cilium-config -n kube-system\n",
    "Find the section related to bpf-lb-sock-hostns-only and set its value to true.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To create a instance group of spot instances, create your cluster as documented above, then update your instance group to use spot instances by performing the following steps:\n",
    "\n",
    "Run kops edit ig <instance-group-name> to edit the instance group config.\n",
    "Add the key-value pair gcpProvisioningModel: SPOT in the instance group spec:\n",
    "\n",
    "spec:\n",
    "  gcpProvisioningModel: SPOT\n",
    "Run kops update cluster --yes followed by kops rolling-update cluster --yes to update the instance group.\n",
    "You can verify this succeeded on the Google Cloud Platform developer console by navigating to Compute Engine, clicking on your particular node instance (by default it will be named something like nodes-<zone>) to pull up instance details, then under Management > Availability Policy there should be a setting that says VM Provisioning Model: Spot.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "kubectl delete crd --selector=app.kubernetes.io/instance=1.5.1 -n kubeflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://charmed-kubeflow.io/docs/get-started-with-charmed-kubeflow#heading--verify-deployment\n",
    "\n",
    "https://charmed-kubeflow.io/docs/install#heading--access-the-dashboard-and-log-in\n",
    "\n",
    "kubectl get services -n kubeflow\n",
    "kubectl get pods -n kubeflow\n",
    "stio-ingressgateway\n",
    "\n",
    "kubectl -n kubeflow get svc istio-ingressgateway-workload -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n",
    "\n",
    "juju config dex-auth public-url=http://35.200.211.213.nip.io\n",
    "juju config oidc-gatekeeper public-url=http://35.200.211.213.nip.io \n",
    "\n",
    "http://35.200.211.213.nip.io\n",
    "\n",
    "juju config dex-auth static-username=admin\n",
    "juju config dex-auth static-password=admin\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "juju add-k8s myk8s\n",
    "\n",
    "juju bootstrap \n",
    "juju add-model kubeflow\n",
    "\n",
    "juju deploy kubeflow --trust\n",
    "\n",
    "kubectl get pods -n kubeflow\n",
    "\n",
    "kubectl -n kubeflow get svc istio-ingressgateway-workload -o jsonpath='{.status.loadBalancer.ingress[0].ip}'\n",
    "\n",
    "juju config dex-auth public-url=http://35.200.211.213.nip.io\n",
    "juju config oidc-gatekeeper public-url=http://35.200.211.213.nip.io \n",
    "\n",
    "http://35.200.211.213.nip.io\n",
    "\n",
    "juju config dex-auth static-username=admin\n",
    "juju config dex-auth static-password=admin\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "helm install --repo https://helm.dask.org --create-namespace -n dask-operator --generate-name dask-kubernetes-operator\n",
    "\n",
    "kubectl get daskclusters\n",
    "\n",
    "kubectl get pods -A -l app.kubernetes.io/name=dask-kubernetes-operator\n",
    "\n",
    "# edit role to access dask cluster\n",
    "kubectl patch clusterrole kubeflow-kubernetes-edit --type=\"json\" --patch '[{\"op\": \"add\", \"path\": \"/rules/-\", \"value\": {\"apiGroups\": [\"kubernetes.dask.org\"],\"resources\": [\"*\"],\"verbs\": [\"*\"]}}]'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "helm repo add spark-operator https://googlecloudplatform.github.io/spark-on-k8s-operator\n",
    "\n",
    "helm install my-release spark-operator/spark-operator --namespace spark-operator --create-namespace\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
